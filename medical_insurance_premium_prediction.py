# -*- coding: utf-8 -*-
"""Medical_Insurance_Premium_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12jpX0bOhv9jQMrQG4grhtM4PLteExdfG

##Importing all necessary libraries
"""

import tensorflow as tf
import numpy as np
import zipfile as zp
import os
import pandas as pd

"""##Mounting Google Drive to Import Dataset from Google Drive

---


"""

from google.colab import drive
import io
drive.mount('/content/drive')

"""##Importing DataSet"""

#Reading and saving the data 
insurance = pd.read_csv("/Medical_premium.csv")
insurance

"""##Visualizing Data"""

#Data values 
insurance.head()

"""##Normalization and Standadization of Data"""

from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler,OneHotEncoder
from sklearn.model_selection import train_test_split

#Create a Column Transformer

ct = make_column_transformer(
    (MinMaxScaler(),["Age","Height","Weight","NumberOfMajorSurgeries"]),
    (OneHotEncoder(handle_unknown = "ignore"),["Diabetes","BloodPressureProblems","AnyTransplants","AnyChronicDiseases","KnownAllergies","HistoryOfCancerInFamily"])
)
#Creating X and Y values
x = insurance.drop("PremiumPrice",axis = 1)
y = insurance["PremiumPrice"]

#Create trainning and test set(use scikit)
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.2,random_state = 42)#20% test size

# Fit the column transformer to our training data
ct.fit(x_train)

#Transform trainning and test data normalization (MinMaxScaler) and one Hot
x_train_normal = ct.transform(x_train) 
x_test_normal = ct.transform(x_test)

x_train_normal.shape,x_train.shape

"""##Steps in modelling with TensorFlow

`1.) Creating a model` - define the input and output layers, as well as the hidden layers of a deep learning model.

`2.) Compiling a model` - define the loss funtion (in other words, the function which tells our model how wrong it is) and the optimizer (tells our model how to improve the patterns its learning) and evaluation metrics (what we can use to interpret the performance of our model).

`3.) Fitting a model` - letting the model try to find patterns between X & y (features and labels).
"""

#Building a Neural Network
tf.random.set_seed(42)

#1. Create the model
model_insurance = tf.keras.Sequential([
    tf.keras.layers.Dense(100,activation="relu"),
    tf.keras.layers.Dense(10,activation="relu"),
    tf.keras.layers.Dense(1)
])

#2. Compile Model
model_insurance.compile(
    loss = tf.keras.losses.mae,
    optimizer = tf.keras.optimizers.Adam(),
    metrics = ["mae"]
)

#3. Fitting the model
model_history = model_insurance.fit(
    x_train_normal,
    y_train,
    epochs = 450,
    verbose=0
)

"""##Plotting History Graph"""

#plot history (also known as the loss curve)
import matplotlib.pyplot as plt
pd.DataFrame(model_history.history).plot()
plt.ylabel("Loss")
plt.xlabel("Epochs")

evaluated_value = model_insurance.evaluate(x_test_normal,y_test)

y_pred = model_insurance.predict(x_test_normal)

pd.DataFrame(model_history.history).plot()

model_insurance.summary()

